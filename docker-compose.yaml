version: "3.8"

x-common-env: &common_env
  HF_HUB_ENABLE_HF_TRANSFER: "1"     # faster downloads
  VLLM_WORKER_MULTIPROC_METHOD: "spawn"
  VLLM_LOGGING_LEVEL: "INFO"
  TRANSFORMERS_OFFLINE: "0"          # set to 1 after first pull if you want fully offline
  CUDA_DEVICE_ORDER: "PCI_BUS_ID"
  OMP_NUM_THREADS: "1"
  VLLM_ATTENTION_BACKEND: "TRITON_ATTN_VLLM_V1"
  PYTORCH_CUDA_ALLOC_CONF: "expandable_segments:True"
  TORCH_CUDA_ARCH_LIST: "8.6"
  CUDA_CACHE_MAXSIZE: "2147483648"               # 2 GiB (bump if you want)

x-common-volumes: &common_volumes
  - /mnt/nas/hf_vllm_models/:/root/.cache/huggingface
  - /mnt/nas/vllm_compile_cache/:/root/.cache/vllm/torch_compile_cache
  - /mnt/nas/gpt-oss-cache/torchinductor/:/root/.cache/torchinductor
  - /mnt/nas/gpt-oss-cache/torch_extensions/:/root/.cache/torch_extensions
  - /mnt/nas/gpt-oss-cache/triton_cache/:/root/.triton
  - /mnt/nas/gpt-oss-cache/nvrtc_cache/:/root/.cache/nvrtc

services:

#######################################
#####    GPT-OSS Models - Text only
#######################################
  # 120B Requires two (2) A6000s with 48 GB VRAM each
  # vllm-gptoss-120b:
  #   image: vllm/vllm-openai:gptoss
  #   container_name: vllm-gptoss-120b
  #   pull_policy: always
  #   runtime: nvidia
  #   shm_size: "64g"
  #   ulimits: { memlock: -1, stack: 67108864 }
  #   environment:
  #     <<: *common_env
  #     CUDA_VISIBLE_DEVICES: "0,2"
  #     TORCHINDUCTOR_CACHE_DIR: "/root/.cache/torchinductor/gptoss-120b"
  #     TORCH_EXTENSIONS_DIR: "/root/.cache/torch_extensions/gptoss-120b"
  #     TRITON_CACHE_DIR: "/root/.triton/gptoss-120b"
  #     CUDA_CACHE_PATH: "/root/.cache/nvrtc/gptoss-120b"
  #   command: >
  #     --model openai/gpt-oss-120b
  #     --served-model-name gpt-oss-120b
  #     --tensor-parallel-size 2
  #     --max-model-len 131072
  #     --gpu-memory-utilization 0.95
  #     --async-scheduling
  #     --swap-space 64
  #     --max-num-seqs 8
  #   ports:
  #     - 8011:8000
  #   volumes: *common_volumes
  #   networks:
  #     - oi_net

  # 20B requires at least 16GB VRAM, running on one (1) A6000
  vllm-gptoss-20b:
    image: vllm/vllm-openai:gptoss
    container_name: vllm-gptoss-20b
    pull_policy: always
    runtime: nvidia
    shm_size: "64g"
    ulimits: { memlock: -1, stack: 67108864 }
    environment:
      <<: *common_env
      CUDA_VISIBLE_DEVICES: "2"
      TORCHINDUCTOR_CACHE_DIR: "/root/.cache/torchinductor/gptoss-20b"
      TORCH_EXTENSIONS_DIR: "/root/.cache/torch_extensions/gptoss-20b"
      TRITON_CACHE_DIR: "/root/.triton/gptoss-20b"
      CUDA_CACHE_PATH: "/root/.cache/nvrtc/gptoss-20b"
    command: >
      --model openai/gpt-oss-20b
      --served-model-name gpt-oss-20b
      --max-model-len 64000
      --gpu-memory-utilization 0.95
      --async-scheduling
      --swap-space 32
      --max-num-seqs 4
    ports:
      - 8012:8000
    volumes: *common_volumes
    networks:
      - oi_net

#######################################
##### Llama 3.2 Model - Vision/Text
#######################################
  # 11B model with 32K Context requires one (1) A6000
  # vllm-llama32-11b-vision:
  #   image: vllm/vllm-openai:latest
  #   container_name: vllm-llama32-11b-vision
  #   pull_policy: always
  #   runtime: nvidia
  #   shm_size: "24g"
  #   ulimits: { memlock: -1, stack: 67108864 }
  #   environment:
  #     <<: *common_env
  #     CUDA_VISIBLE_DEVICES: "0"
  #     TORCH_CUDA_ARCH_LIST: "8.6"
  #   command: >
  #     --model meta-llama/Llama-3.2-11B-Vision-Instruct
  #     --served-model-name llama-3.2-11b-vision
  #     --max-model-len 32000
  #     --gpu-memory-utilization 0.95
  #     --max-num-seqs 2
  #   ports: [ "8015:8000" ]
  #   volumes: *common_volumes
  #   networks: [ oi_net ]

#######################################
##### ComfyUI Model - Image Generation
#######################################
  comfyui:
    container_name: comfyui
    image: frefrik/comfyui-flux:cu124
    pull_policy: always
    restart: unless-stopped
    runtime: nvidia
    ports:
      - "8188:8188"
    volumes:
      - /mnt/nas/hf_comfyui_models/:/app
    environment:
      - CLI_ARGS=
      - HF_TOKEN=${HUGGING_TOKEN2}
      - LOW_VRAM=${LOW_VRAM:-false}
      - CUDA_VISIBLE_DEVICES=0
      - MODELS_DOWNLOAD="dev"
    deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                device_ids: [ '0' ]
                capabilities: [ gpu ]
    networks:
      - oi_net

#######################################
##### Ollama - Non-Compatible vLLM Models
#######################################
  # ollama:
  #   volumes:
  #     - /mnt/nas/ollama_webui/ollama:/root/.ollama
  #   container_name: ollama
  #   pull_policy: always
  #   restart: always
  #   tty: true
  #   image: ollama/ollama:${OLLAMA_DOCKER_TAG-latest}
  #   ports:
  #     - ${OLLAMA_WEBAPI_PORT-11434}:11434
  #   runtime: nvidia
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: [ '1' ]
  #             capabilities:
  #               - gpu
  #   networks:
  #     - oi_net

#######################################
##### Apache Tika - Document/OCR Extraction
#######################################
  tika:
    image: apache/tika:latest-full
    restart: always
    ports:
      - 8009:9998
    networks:
      - oi_net

#######################################
##### Open WebUI - Chat Interface
#######################################
  open-webui:
    image: ghcr.io/open-webui/open-webui:${WEBUI_DOCKER_TAG-main}
    container_name: open-webui
    pull_policy: always
    restart: always
    volumes:
      - /mnt/nas/ollama_webui/webui:/app/backend/data
    ports:
      - 8010:8080
    environment:
      - WEBUI_SECRET_KEY=""
      - CONTENT_EXTRACTION_ENGINE=tika
      - ENABLE_IMAGE_GENERATION=True
      - IMAGE_GENERATION_ENGINE=comfyui
      - COMFYUI_BASE_URL=http://comfyui:8188
      - IMAGE_SIZE=1024x1024
      - IMAGE_STEPS=20
      - OPENAI_API_BASE_URL=http://vllm-gptoss-120b:8000/v1;http://vllm-gptoss-20b:8000/v1
      - OPENAI_API_KEY=EMPTY;EMPTY
    networks:
      - oi_net

networks:
  oi_net:
    driver: bridge
